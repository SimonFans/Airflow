1. depends_on_past && wait_for_downstream

depends_on_past: (boolean) when set to True, keeps a task from getting triggered if the previous schedule for the task hasnâ€™t succeeded. only apply for a single task
Example: if you set epends_on_past=True in the task C, then Task C in the Dag run2 will only run when Task C in the Dag run1 succeed or skipped.
Dag run1: A-> B-> C
Dag run2: A-> B-> C

wait_for_downstream: (boolean) when set to True, it set depends_on_past=True by default. will prevent downstream tasks of the current dagrun from running if the upstream task instances of previous dagrun are not in the success state.
Example:if you set wait_for_downstream=true in the Task A, then Task A in the Dag run2 will not run until the Task A and its downstream Task B in the Dag run1 has succeed or skipped
Dag run1: A-> B-> C
Dag run2: A-> B-> C

2. Inside the docker web-server container, you can turn on/off the toggle to pause & unpause the dags
command: airflow dags pause <dag_id>
command: airflow dags unpause <dag_id>

3. Trigger your dag in the docker container
command: airflow dags trigger -e

4. List your dags
command: airflow dags list

5. List tasks in airflow Dags:
command: airflow tasks list example_dag

6. Test your dags
If you add a new task there, you can run below command to test
command: airflow tasks test example_dag bash_print_date1 2021-01-01

7. Rerun your past dag runs (backfill)
command: airflow dags backfill -s 2021-01-01 -e 2021-01-05 --reset-dagruns example_dag

8. Pool
definition: the pool allows you to limit concurrency for a set of tasks by default. With Airflow, you can run up to 32 tasks at the same time. 
use case: when some specific tasks that consuming running very long time. You might want to limit concurrency for those tasks to 1 to execute them sequentially, one after the other.
Task running = 1 worker slot taken
All tasks go to default_pool (128 slots), you can find it from Admin -> pool
How to use it:
If you want to run some big tasks one by one, then go to set up a new pool. Set the slots to be 1, which means all tasks will take up only one slots (run 1 by 1). Add pool pararmeter into the Operator such as pool = "<pool name>"

9. Use cross_downstream to solve the task dependencies between two lists 
Airflow doesn't allow: such as [A,B] >> [C,D,E] >> F
So you need to import:
from airflow.utils.helpers import cross_downstream
cross_downstream([A,B], [C,D,E])
[C,D,E] >> F

10. Task priority:
You want to prioritize one task to go first. Add priority_weight parameter into your operator. But make sure all tasks should be in the same pool. The default value of priority_weight is 1.
Example:
process_a = BashOperator(
            task_id = 'process_a',
            bash_command="echo '{{ ti.try_number }}' && sleep 20",
            priority_weight=2,
            pool="process_tasks"
)
process_b = BashOperator(
            task_id = 'process_b',
            bash_command="echo '{{ ti.try_number }}' && sleep 20",
            pool="process_tasks"
)
process_c = BashOperator(
            task_id = 'process_c',
            bash_command="echo '{{ ti.try_number }}' && sleep 20",
            priority_weight=3,
            pool="process_tasks"
)
You might have different numbers but the point is, the priority_weights should be set as follow:
process_c > process_a > process_b
Notice that by default, process_b has a priority_weight equals to 1 so no need to specify it here.

11. Trigger_rules
- all_success
Example: [t1, t2] >> t3
Explain: If add rule to t3, then t3 will be triggered only when t1 and t2 are succcessful. If t2 is skipped, then t3 will also be skipped.

- all_failed
Explain: All upstreaming tasks failed then triggered otherwise skip.

- all_done
Explain: not care the upstreaming tasks state just trigger to run when upstreaming tasks are done

- one_failed
- one_success
- non_failed
Example: [t1, t2] >> t3
Explain: if t1 and t2 are skipped, then t3 trigger.
- non_failed_or_skipped
Example: [t1, t2] >> t3
Explain: if t1 and t2 are skipped, then t3 skip.
- non_skipped





