1. depends_on_past && wait_for_downstream

depends_on_past: (boolean) when set to True, keeps a task from getting triggered if the previous schedule for the task hasnâ€™t succeeded. only apply for a single task
Example: if you set epends_on_past=True in the task C, then Task C in the Dag run2 will only run when Task C in the Dag run1 succeed or skipped.
Dag run1: A-> B-> C
Dag run2: A-> B-> C

wait_for_downstream: (boolean) when set to True, it set depends_on_past=True by default. will prevent downstream tasks of the current dagrun from running if the upstream task instances of previous dagrun are not in the success state.
Example:if you set wait_for_downstream=true in the Task A, then Task A in the Dag run2 will not run until the Task A and its downstream Task B in the Dag run1 has succeed or skipped
Dag run1: A-> B-> C
Dag run2: A-> B-> C

2. Inside the docker web-server container, you can turn on/off the toggle to pause & unpause the dags
command: airflow dags pause <dag_id>
command: airflow dags unpause <dag_id>

3. Trigger your dag in the docker container
command: airflow dags trigger -e

4. List your dags
command: airflow dags list

5. List tasks in airflow Dags:
command: airflow tasks list example_dag

6. Test your dags
If you add a new task there, you can run below command to test
command: airflow tasks test example_dag bash_print_date1 2021-01-01

7. Rerun your past dag runs (backfill)
command: airflow dags backfill -s 2021-01-01 -e 2021-01-05 --reset-dagruns example_dag

8. Pool
definition: the pool allows you to limit concurrency for a set of tasks by default. With Airflow, you can run up to 32 tasks at the same time. 
use case: when some specific tasks that consuming running very long time. You might want to limit concurrency for those tasks to 1 to execute them sequentially, one after the other.
Task running = 1 worker slot taken
All tasks go to default_pool (128 slots), you can find it from Admin -> pool
How to use it:
If you want to run some big tasks one by one, then go to set up a new pool. Set the slots to be 1, which means all tasks will take up only one slots (run 1 by 1). Add pool pararmeter into the Operator such as pool = "<pool name>"

9. Use cross_downstream to solve the task dependencies between two lists 
Airflow doesn't allow: such as [A,B] >> [C,D,E] >> F
So you need to import:
from airflow.utils.helpers import cross_downstream
cross_downstream([A,B], [C,D,E])
[C,D,E] >> F





